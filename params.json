{"name":"RunJOP","tagline":"RunJOP (Run Just Once Please) is a distributed execution framework to run a command (i.e. a job) only once in a group of servers and can be used together with UNIX/Linux cron to put a crontab schedule in High Availability (HA).","body":"### Run Just Once Please: runjop\r\n\r\nRunJOP (Run Just Once Please) is a distributed execution framework to run a command (i.e. a job) only once in a group of servers and can be used together with UNIX/Linux [cron](http://en.wikipedia.org/wiki/Cron) to put a crontab schedule in High Availability (HA).\r\n\r\nAnother possible use case is to execute a command after an SNS notification is received in HA on multiple nodes, using the `MessageId` as the id of the job to make sure is executed only once.\r\n\r\n\r\n* The idea is to use [Amazon DynamoDB](http://aws.amazon.com/dynamodb/) to make sure only one server \"reserves\" the right to execute the command for a certain range of time.\r\n* [Amazon S3](http://aws.amazon.com/s3/) can optionally be used to consolidate the logs of the jobs in a single repository.\r\n* AWS credentials can be passed using AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environmental variables.\r\n* In an EC2 instance a IAM role can be used to give access to DynamoDB/S3 resources.\r\n\r\n**This is a personal project. No relation whatsoever exists between this project and my employer.**\r\n\r\n### License\r\n\r\nCopyright (c) 2013 Danilo Poccia, `http://blog.danilopoccia.net`\r\n\r\nThis code is licensed under the The MIT License (MIT). Please see the LICENSE file that accompanies this project for the terms of use.\r\n\r\n### Introduction\r\n\r\nRunning this two commands concurrently on two hosts one of the node will execute the command, the other will not. In this example the command is executed on the \"second\" node. Debugging info is added to give more information on the execution.\r\n\r\nIn this example, on the \"first\" node the \"Hello World\" command is not executed:\r\n\r\n    runjop.py --region=eu-west-1 --table myschedule --id my-job --range=10 --node first --s3=s3://BUCKET/mylogs \"echo Hello World\" --log /tmp/runjop.log -d\r\n    DEBUG:runjop:__init__ '{'node': 'first', 's3log': 's3://BUCKET/mylogs', 'region': 'eu-west-1', 'range': '10', 'debug': True, 'table': 'runjop', 'logfile': '/tmp/runjop.log', 'id': 'my-job'}'\r\n    INFO:runjop:S3 bucket: 'BUCKET'\r\n    INFO:runjop:S3 prefix: 'mylogs/'\r\n    DEBUG:runjop:table 'runjop' not found\r\n    INFO:runjop:table 'runjop' created\r\n    DEBUG:runjop:waiting for table 'runjop' to be active\r\n    DEBUG:runjop:table 'runjop' is active\r\n    DEBUG:runjop:run '['echo Hello World']'\r\n    DEBUG:runjop:now = '2013-02-11 16:03:46'\r\n    DEBUG:runjop:last_item = '{u'node': u'second', u'counter': 1, u'job_id': u'my-ls', u'time': u'2013-02-11 16:03:46'}'\r\n    DEBUG:runjop:last_time_str = '2013-02-11 16:03:46'\r\n    DEBUG:runjop:counter = '1'\r\n    DEBUG:runjop:outside of range of 10 seconds: False\r\n    INFO:runjop:not outside of range of execution\r\n    INFO:runjop:command not executed\r\n\r\nOn the \"second\" node, the \"Hello World\" command is executed:\r\n\r\n    runjop.py --region=eu-west-1 --table myschedule --id my-job --range=10 --node second --s3=s3://BUCKET/mylogs \"echo Hello World\" --log /tmp/runjop.log -d\r\n    DEBUG:runjop:__init__ '{'node': 'second', 's3log': 's3://BUCKET/mylogs', 'region': 'eu-west-1', 'range': '10', 'debug': True, 'table': 'runjop', 'logfile': '/tmp/runjop.log', 'id': 'my-job'}'\r\n    INFO:runjop:S3 bucket: 'BUCKET'\r\n    INFO:runjop:S3 prefix: 'mylogs/'\r\n    DEBUG:runjop:table 'runjop' found\r\n    DEBUG:runjop:waiting for table 'runjop' to be active\r\n    DEBUG:runjop:table 'runjop' is active\r\n    DEBUG:runjop:run '['echo Hello World']'\r\n    DEBUG:runjop:now = '2013-02-11 16:03:46'\r\n    DEBUG:runjop:outside of range of 10 seconds: True\r\n    DEBUG:runjop:put result '{u'ConsumedCapacityUnits': 1.0}'\r\n    DEBUG:runjop:execute_job 'True'\r\n    INFO:runjop:executing command 'echo Hello World'\r\n    INFO:runjop:returncode = 0\r\n    INFO:runjop:output:\r\n    Hello World\r\n\r\n    INFO:runjop:output written on s3://danilop-fs/logs/runjop-my-ls-20130211-160346-second-0.log\r\n\r\nOn DynamoDB the \"myschedule\" table can be used as an activity log:\r\n\r\n    job_id    counter  node      time \r\n    \"my-job\"  1        \"second\"  \"2013-02-11 16:03:46\"\r\n    \"my-job\"  2        \"first\"   \"2013-02-11 16:08:52\"\r\n\r\nThe optional S3 log has the following naming convention:\r\n\r\n    {table}-{id}-{YYYYMMDD}-{hhmmss}-{node}.log\r\n\r\n### Using with cron\r\n\r\nThe previous example can be scheduled using [cron](http://en.wikipedia.org/wiki/Cron) on more than one hosts, but only one will actually run it.\r\n\r\nIn this example two options are removed from the invocation of the tool (compared to the previous one):\r\n* without the \"--node\" option the hostname of each node is used\r\n* without the \"--range\" option the default 300 seconds (5 minutes) range is used.\r\n\r\nE.g. to execute the job one minute past midnight (00:01) of every day of the month, of every day of the week:\r\n\r\n    1 0 * * *  /somepath/runjop.py --region=eu-west-1 --table myschedule --id my-job --range=10 --s3=s3://BUCKET/mylogs \"echo Hello World\" --log /var/log/runjop.log\r\n\r\nE.g. to\texecute\tthe job to be run every two hours, namely at midnight, 2am, 4am, 6am, 8am, and so on:\r\n\r\n    0 */2 * * *  /home/username/runjop.py --region=eu-west-1 --table myschedule --id my-job --range=10 --s3=s3://BUCKET/mylogs \"echo Hello World\" --log /var/log/runjop.log\r\n\r\n### Full Usage\r\n\r\n    Usage: runjop.py [options] \"<command(s)>\"\r\n\r\n    RunJOP (Run Just Once Please)\r\n\r\n    A distributed execution framework to run a command (i.e. a job) only once in a group of servers.\r\n    This can be used together with UNIX/Linux cron to put a crontab schedule in High Availability (HA).\r\n    The idea is to use Amazon DynamoDB to make sure only one server \"reserves\" the right\r\n    to execute the command for a certain range of time.\r\n    Amazon S3 can optionally be used to consolidate the logs of the jobs in a single repository.\r\n\r\n    Options:\r\n      -h, --help       show this help message and exit\r\n      --region=REGION  AWS region to use for DynamoDB (default is us-east-1)\r\n      --table=TABLE    the DynamoDB table use to check concurrency and log job\r\n\t\t       executions (a new table is created if not found)\r\n      --id=ID          the unique ID identifying this job across multiple servers\r\n      --node=NODE      an identifier for the node (default on this node is\r\n\t\t       current 'hostname')\r\n      --range=S        the range of time (in seconds) in which the execution of\r\n\t\t       the job must be unique (default is 300 seconds)\r\n      --s3=URL         the optional S3 path to put the output of the job in\r\n\t\t       s3://BUCKET[/PATH] format\r\n      --log=FILE       the local filename to use for logs\r\n      -d, --debug      print debug information\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}